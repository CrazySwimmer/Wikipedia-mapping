
%preamble
\documentclass[10pt]{article}


\usepackage{times}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{stmaryrd}
\usepackage{soul}
\usepackage{booktabs}

\title{Wikipedia Crawler}
\author{Danilo Zocco\\ 
	\and Nick Stoeckl\\ 	
	\and Patrick Waelchli\\ 
	\and Gian-Reto Bonadurer\\ 
	\\ \\
	\small Software Engineering for Economists\\
	\small Philipp Zahn\\
	\small University of St. Gallen}

\date{January 5, 2018}

%Rename Contents into Table of Contents
\renewcommand*\contentsname{Table of Contents}

%Document Start

\begin{document}
\maketitle
\newpage


%Table of Content
\tableofcontents

\newpage

% Content
\section{Introduction}

The amount of accessible data increases everyday. Naturally this places ever greater importance on Data science as it is a proven method of processing such vast numbers of available data points. Gathering and processing information automatically from 
the internet is becoming more crucial than ever. Whether it is in order to gain a competitive advantage, to conduct extensive market research, or to simply gather information it is a fundamental advantage to work with algorithms that replace manual labour. 
This project illustrates, in a simplified manner, one way data science has the ability to tackle such problems. The build crawler gathers the available links within a Wikipedia article and subsequently visualizes its findings through a network-map. 
There are different approaches on how the crawler selects the links within an article, which will be discussed in further detail in Chapter 4.

\section{Language}

The general-purpose programming language Python is widely used for data science worldwide, the reason being that the language allows a broad variety of application forms. With regards to this project, Python is not the only language that could be used to build a crawler but it seemed to be the most suitable as the syntax of Python and the prebuilt modules allow an uncomplicated use. The availability of these prebuilt modules enabled us to choose the most suitable one for the project and thus allowed us to place more focus on the customisation of the crawler. Finally, because the team members want to use Python in the future, the choice for the lastest version of Python (3) was clear. Although the team thought the decreased availability of forum articles might lead to an unreasonable amount of effort to solve prevailing problems, it later became apparent that available online resources were more than sufficient to solve the task. 


\section{Modules}

 \begin{itemize}
        \item \textbf{Beautifulsoup 4}\\
 The Beautifulsoup4 is used as a parser.
        \item \textbf{Request}
        \item \textbf{Networkx}
        \item \textbf{Matplotlib}
        \item \textbf{Numpy}
    \end{itemize}
 

\section{Crawler}
\subsection{Extraction from the internet}
\subsection{Graphing the network}
\section{Conclusion}



% References and End of Paper 
%\bibliography{Bibliography-File} 
%\bibliographystyle{aaai}

\end{document}
