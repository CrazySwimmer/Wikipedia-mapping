
%preamble
\documentclass[10pt]{article}

\usepackage[english]{babel}
\usepackage{times}
\usepackage{graphicx}
\graphicspath{ {images/} }

%Titelpage
\title{Wikipedia Crawler}
\author{Danilo Zocco\\ \texttt{11-510-013}
	\and Gian-Reto Bonadurer\\ \texttt{17-605-205}
	\and Nick Stoeckl\\ \texttt{13-605-506}
	\and Patrick Waelchli\\ \texttt{17-601-873}\\ \\
	\large Software Engineering for Economists\\
	\large Philipp Zahn\\
	\large University of St. Gallen}

\date{January 5, 2018}

%Rename Contents into Table of Contents
\renewcommand*\contentsname{Table of Contents}

%Document Start\begin{document}
\maketitle
\newpage

\tableofcontents
\listoffigures
\newpage

% Content

\section{Introduction}

The amount of accessible data increases everyday. Naturally this places ever greater importance on data science as it is a proven method of processing such vast numbers of available data points. \cite{dat} Gathering and processing information automatically from 
the internet is becoming ever more crucial as we delve deeper into the age of digitization. Whether it is in order to gain a competitive advantage, to conduct extensive market research, or to simply gather information the power of data science can be implemented as a fundamental advantage to replace manual labour with algorithms. \par
 \noindent This project illustrates, in a simplified manner, one way data science has the ability to tackle problems like these. The build crawler gathers the available links within a Wikipedia article and subsequently visualizes its findings through a network-map. Although various methods exist for achieving the aforementioned goal, the next sections outlines this team's approach along with their reasoning behind the decisions.

\section{Tools}	
	\subsection{Language}

The general-purpose programming language Python is widely used for data science worldwide, the reason being that the language allows for a broad variety of application forms. \cite{pyt} With regards to this project, Python is by no means the only language that could be used to build a crawler but it seemed to be the most suitable as the syntax of Python and the prebuilt modules allow an uncomplicated use. The availability of these prebuilt modules enabled the team to choose the most suitable one for the project and thus allowed them to place more focus on the customisation of the crawler itself. \par 
 
\noindent Finally, because the team members expect to use Python in the future, the choice for the language's lastest version (Python 3) was clear. while, at first the team thought the limited availability of forum articles might lead to an unreasonable amount of effort to solve prevailing problems, it later became apparent that available online resources were more than sufficient to solve the task. 




\subsection{Modules}

 \begin{itemize}    %Creating bullet points        \item\textbf{Beautiful Soup}\\
	Beautiful Soup (bs4) is a Python library designed for quick turnaround projects like screen-scraping. It allows for an easy way to navigate, search, and modify a parse tree, making  	it easy to extract links from the Wikipedia page without using much code. \cite{cru} 
         \item \textbf{Request}\\
 	The Request module enables communication with HTTP. As a result, the request module in Python does not need any manual input and automatically adds query strings to the  	
	URL.   \cite{req}
	        \item \textbf{Networkx}\\
	``NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.'' \cite{net} This module is used to create 
	the network, which is subsequently visualised as a map. 
	
        \item \textbf{Matplotlib}\\
        The generated network with the networkx module is plotted and visualised with the Matplot library. This tools allows for the creation of different plot types and networks with a high 	
        degree of customizability, if needed. The default version is used for plotting as it consistently produces the most readable network graph. 
        
        \item \textbf{Numpy}\\
        
        (zocco here, please)
        
            \end{itemize}
 

\section{Program}
The program is subdivided into two main parts: a crawler that gets links from Wikipedia and a builder that takes those links, builds a network and finally generates a chart of it.

	\subsection{Crawler}
	The function of a crawler is to access a webpage and to retrieve the hyperlinks listed on it. It then accesses those new links and repeat the process for each and every page it 	
	visits. This is not to be confused with a scrapper that proceeds almost in the same way but will usually gather other pieces of information such as pictures or text.
	\par \noindent
	We faced one main issue while building the crawler. In order not to end up crawling the entire Wikipedia website, we set a limit to the number of items crawled. As a result, 	
	depending on the starting article we choose, the crawler hits this limit after only a few pages. To fix this, we can either increase the maximum items we want to crawl (which also 
	increase the running time) or choose to crawl by depth instead of breadth. In the second option, instead of crawling every link on a page and then moving to the next page, the 
	crawler will always take the first internal link found in an article and directly start crawling the corresponding new article. This allows for more pages to be crawled before reaching 
	the maximum items limit and for a less clutterd network to be built and graphed.\par \noindent

	To illustrate the difference between "breadth-first" and "depth-first" search, we included one graph for each. They are both built with "University of St. Gallen" as starting article and 	have a limit of 15 and 20 items, respectively. Figure 1 illustrates breadth-first search while Figure 2 illustrates depth-first search.\\
	
		\begin{figure}	
			\includegraphics[width=\textwidth]{figure1.png}
    			 	\centering
    			 	\caption{Breadth-first}
\par\par			
			\includegraphics[width=\textwidth]{figure2.png}
   			  	\centering
  			  	\caption{Depth-first}
		\end{figure}
	
	
	\subsection{Building and drawing the network}
	The input to this part is a dictionary of Wikipedia articles and their corresponding internal links. Before building the network object with the NetworkX module, we use NumPy and 	itertools to suppress the pages that would become end nodes (i.e articles linked to one and only one other article). These end nodes do not add much information to the network 	and are often hardly readable on the graph produced. (See Figure 3 for an example.)
	\par \noindent
	Once this is done, we use the NetworkX module to build a networkx.Graph object. These are composed of nodes (articles) and edges (links between articles). Finally, we are able 	to graph the network with Matplotlib.\par \noindent
	
	\begin{figure}[htb]	
			\includegraphics[width=\textwidth]{figure3.png}
   			  	\centering
  			  	\caption{Overloaded graph}
		\end{figure}

	
	
\section{Conclusion}

(after everything is written someone here, please)

% References & End of document
\newpage
\bibliographystyle{apacite} 
\bibliography{wiki_crawl}

\end{document}